{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install pythainlp huggingface_hub sentencepiece\n",
        "!pip install torch==2.1.2 torchvision==0.16.2 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install fastai==2.7.13 transformers==4.41.0 datasets rouge_score\n",
        "!pip install --force-reinstall numpy==1.24.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Bp8YwjxmzX6L",
        "outputId": "6c704990-db3a-4565-983e-0fac9125fbec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pythainlp\n",
            "  Downloading pythainlp-5.1.1-py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.30.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.11/dist-packages (from pythainlp) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->pythainlp) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->pythainlp) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->pythainlp) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.31->pythainlp) (2025.1.31)\n",
            "Downloading pythainlp-5.1.1-py3-none-any.whl (19.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.3/19.3 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pythainlp\n",
            "Successfully installed pythainlp-5.1.1\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu121\n",
            "Collecting torch==2.1.2\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torch-2.1.2%2Bcu121-cp311-cp311-linux_x86_64.whl (2200.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 GB\u001b[0m \u001b[31m719.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.2\n",
            "  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.16.2%2Bcu121-cp311-cp311-linux_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m104.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.2) (2025.3.2)\n",
            "Collecting triton==2.1.0 (from torch==2.1.2)\n",
            "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (2.32.3)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.16.2) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.2) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchvision==0.16.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.2) (1.3.0)\n",
            "Installing collected packages: triton, torch, torchvision\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.21.0+cu124\n",
            "    Uninstalling torchvision-0.21.0+cu124:\n",
            "      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.1.2+cu121 torchvision-0.16.2+cu121 triton-2.1.0\n",
            "Collecting fastai==2.7.13\n",
            "  Downloading fastai-2.7.13-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting transformers==4.41.0\n",
            "  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (24.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (24.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (0.0.7)\n",
            "Collecting fastcore<1.6,>=1.5.29 (from fastai==2.7.13)\n",
            "  Downloading fastcore-1.5.55-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (0.16.2+cu121)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (2.32.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (6.0.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (1.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (11.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (1.14.1)\n",
            "Requirement already satisfied: spacy<4 in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (3.8.5)\n",
            "Requirement already satisfied: torch<2.2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from fastai==2.7.13) (2.1.2+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (2024.11.6)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.41.0)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.41.0) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.23.0->transformers==4.41.0) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->fastai==2.7.13) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->fastai==2.7.13) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->fastai==2.7.13) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->fastai==2.7.13) (2025.1.31)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (0.15.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (2.11.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (75.2.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<4->fastai==2.7.13) (3.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch<2.2,>=1.10->fastai==2.7.13) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<2.2,>=1.10->fastai==2.7.13) (3.4.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.2,>=1.10->fastai==2.7.13) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai==2.7.13) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai==2.7.13) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai==2.7.13) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai==2.7.13) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai==2.7.13) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->fastai==2.7.13) (2.8.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->fastai==2.7.13) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->fastai==2.7.13) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->fastai==2.7.13) (3.6.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<4->fastai==2.7.13) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai==2.7.13) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai==2.7.13) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<4->fastai==2.7.13) (0.4.0)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4->fastai==2.7.13) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy<4->fastai==2.7.13) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai==2.7.13) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<4->fastai==2.7.13) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai==2.7.13) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<4->fastai==2.7.13) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<4->fastai==2.7.13) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch<2.2,>=1.10->fastai==2.7.13) (1.3.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<4->fastai==2.7.13) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai==2.7.13) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai==2.7.13) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<4->fastai==2.7.13) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<4->fastai==2.7.13) (0.1.2)\n",
            "Downloading fastai-2.7.13-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.2/232.2 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.41.0-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastcore-1.5.55-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.2/77.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m113.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=307f08ee7ba46c1daa5208b1c064047d487ec099ad58dbd5411d5a61d6c4a320\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: xxhash, fsspec, fastcore, dill, rouge_score, multiprocess, tokenizers, transformers, datasets, fastai\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: fastcore\n",
            "    Found existing installation: fastcore 1.7.29\n",
            "    Uninstalling fastcore-1.7.29:\n",
            "      Successfully uninstalled fastcore-1.7.29\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: fastai\n",
            "    Found existing installation: fastai 2.7.19\n",
            "    Uninstalling fastai-2.7.19:\n",
            "      Successfully uninstalled fastai-2.7.19\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.2+cu121 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fastai-2.7.13 fastcore-1.5.55 fsspec-2024.12.0 multiprocess-0.70.16 rouge_score-0.1.2 tokenizers-0.19.1 transformers-4.41.0 xxhash-3.5.0\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "35da2a3a31cf4df48fade4a4fb6fd650"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-BSkg2lzB3M",
        "outputId": "a4b8519d-7219-4bc6-bf47-68993aa39e5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 50 documents\n",
            "Warning: Informal marker 'นะ' detected\n",
            "Loaded 50 clean pairs\n",
            "Tokenized example: ['xxbos', 'เอกสาร', 'นี้', 'ระบุ', 'ว่า', 'ตามที่', 'ได้', 'หารือ', 'กัน', 'คะ', 'xxeos']\n",
            "Tokenizing texts...\n",
            "Creating vocabulary...\n",
            "\n",
            "Success! DataLoaders created with:\n",
            "- Vocab size: 2067\n",
            "- Input shape: torch.Size([64])\n",
            "- Target shape: torch.Size([64])\n",
            "Input shape: torch.Size([4, 64])\n",
            "Target shape: torch.Size([4, 64])\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from fastai.text.all import *\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "import warnings\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "### 1. DATA LOADING & VALIDATION ###############################################\n",
        "def load_and_validate_data(csv_path):\n",
        "    \"\"\"Load data with strict validation\"\"\"\n",
        "    df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
        "\n",
        "    # Validate structure\n",
        "    assert {'text', 'summary'}.issubset(df.columns), \"Missing required columns\"\n",
        "\n",
        "    # Clean data\n",
        "    df = df.dropna(subset=['text', 'summary'])\n",
        "    df = df[(df['text'].str.len() > 30) & (df['summary'].str.len() > 15)]\n",
        "    df = df.reset_index(drop=True)\n",
        "    print(f\"Loaded {len(df)} documents\")\n",
        "\n",
        "    # Formal language checks\n",
        "    informal_markers = ['ครับ', 'ค่ะ', 'นะ']\n",
        "    for marker in informal_markers:\n",
        "        if df['summary'].str.contains(marker).any():\n",
        "            print(f\"Warning: Informal marker '{marker}' detected\")\n",
        "\n",
        "    print(f\"Loaded {len(df)} clean pairs\")\n",
        "    return df\n",
        "\n",
        "# Load data\n",
        "csv_path = \"formal_thai.csv\"\n",
        "df = load_and_validate_data(csv_path)\n",
        "\n",
        "### 2. CUSTOM TOKENIZER ########################################################\n",
        "class ThaiTokenizer:\n",
        "    def __init__(self):\n",
        "        self.special_toks = ['xxbos', 'xxeos', 'xxpad', 'xxunk']\n",
        "        self.formal_map = {'ครับ': 'คะ', 'ค่ะ': 'คะ', 'นะครับ': 'คะ'}\n",
        "\n",
        "    def __call__(self, text, **kwargs):\n",
        "        tokens = word_tokenize(str(text), engine=\"newmm\")\n",
        "        tokens = [self.formal_map.get(t, t) for t in tokens if t.strip()]\n",
        "        return ['xxbos'] + tokens + ['xxeos']\n",
        "\n",
        "tokenizer = ThaiTokenizer()\n",
        "\n",
        "# Test tokenizer\n",
        "sample_text = \"เอกสารนี้ระบุว่า ตามที่ได้หารือกันครับ\"\n",
        "print(\"Tokenized example:\", tokenizer(sample_text))\n",
        "\n",
        "### 3. DATA PREPARATION & DATALOADER ##########################################\n",
        "def prepare_dataloaders(df, seq_len=64, bs=4):\n",
        "    print(\"Tokenizing texts...\")\n",
        "    sources = [tokenizer(str(t)) for t in df['text']]\n",
        "    targets = [tokenizer(str(t)) for t in df['summary']]\n",
        "\n",
        "    print(\"Creating vocabulary...\")\n",
        "    all_tokens = [tok for sublist in sources+targets for tok in sublist]\n",
        "    counter = Counter(all_tokens)\n",
        "    vocab = ['xxbos', 'xxeos', 'xxpad', 'xxunk'] + [tok for tok, cnt in counter.most_common(30000)]\n",
        "    word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "    pad_idx = word2idx['xxpad']\n",
        "    unk_idx = word2idx['xxunk']\n",
        "\n",
        "    # Save vocabulary\n",
        "    with open(\"thai_vocab.pkl\", \"wb\") as f:\n",
        "        pickle.dump(word2idx, f)\n",
        "\n",
        "    # Convert tokens to indices with fallback to xxunk\n",
        "    def to_idx(seq, word2idx):\n",
        "        return [word2idx.get(w, unk_idx) for w in seq]\n",
        "\n",
        "    def pad_or_trim(t, length, pad_idx):\n",
        "        if len(t) < length:\n",
        "            return F.pad(t, (0, length - len(t)), value=pad_idx)\n",
        "        else:\n",
        "            return t[:length]\n",
        "\n",
        "    src_nums = [pad_or_trim(torch.tensor(to_idx(seq, word2idx)), seq_len, pad_idx) for seq in sources]\n",
        "    tgt_nums = [pad_or_trim(torch.tensor(to_idx(seq, word2idx)), seq_len, pad_idx) for seq in targets]\n",
        "\n",
        "    # Create Datasets and DataLoaders\n",
        "    padded_src = torch.stack(src_nums)\n",
        "    padded_tgt = torch.stack(tgt_nums)\n",
        "    train_ds = TensorDataset(padded_src, padded_tgt)\n",
        "\n",
        "    dls = DataLoaders.from_dsets(\n",
        "        train_ds,\n",
        "        valid_pct=0.2,\n",
        "        bs=bs,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    print(\"\\nSuccess! DataLoaders created with:\")\n",
        "    print(f\"- Vocab size: {len(vocab)}\")\n",
        "    print(f\"- Input shape: {padded_src[0].shape}\")\n",
        "    print(f\"- Target shape: {padded_tgt[0].shape}\")\n",
        "    return dls\n",
        "\n",
        "# Create and verify DataLoaders\n",
        "dls = prepare_dataloaders(df)\n",
        "\n",
        "# Verify one batch\n",
        "x, y = dls.one_batch()\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Target shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pythainlp.tokenize import word_tokenize\n",
        "from fastai.text.all import *\n",
        "from rouge_score import rouge_scorer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "import pickle\n",
        "from collections import Counter\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "### 1. LOAD & CLEAN DATA ##################################################\n",
        "def load_and_validate_data(csv_path):\n",
        "    df = pd.read_csv(csv_path, encoding='utf-8-sig')\n",
        "    assert {'text', 'summary'}.issubset(df.columns), \"Missing required columns\"\n",
        "    df = df.dropna(subset=['text', 'summary'])\n",
        "    df = df[(df['text'].str.len() > 30) & (df['summary'].str.len() > 15)]\n",
        "    df = df.reset_index(drop=True)\n",
        "\n",
        "    # Check for informal tone\n",
        "    informal_markers = ['ครับ', 'ค่ะ', 'นะ']\n",
        "    for marker in informal_markers:\n",
        "        if df['summary'].str.contains(marker).any():\n",
        "            print(f\"Warning: Informal marker '{marker}' detected in summaries\")\n",
        "\n",
        "    print(f\"Loaded {len(df)} clean pairs\")\n",
        "    return df\n",
        "\n",
        "csv_path = \"formal_thai.csv\"\n",
        "df = load_and_validate_data(csv_path)\n",
        "\n",
        "### 2. THAI TOKENIZER ######################################################\n",
        "class ThaiTokenizer:\n",
        "    def __init__(self):\n",
        "        self.special_toks = ['xxbos', 'xxeos', 'xxpad', 'xxunk']\n",
        "        self.formal_map = {'ครับ': 'คะ', 'ค่ะ': 'คะ', 'นะครับ': 'คะ'}\n",
        "\n",
        "    def __call__(self, text, **kwargs):\n",
        "        tokens = word_tokenize(str(text), engine=\"newmm\")\n",
        "        tokens = [self.formal_map.get(t, t) for t in tokens if t.strip()]\n",
        "        return ['xxbos'] + tokens + ['xxeos']\n",
        "\n",
        "tokenizer = ThaiTokenizer()\n",
        "\n",
        "### 3. DATASET & DATALOADER PREP ############################################\n",
        "def prepare_dataloaders(df, seq_len=64, bs=4):\n",
        "    print(\"Tokenizing texts...\")\n",
        "    sources = [tokenizer(str(t)) for t in df['text']]\n",
        "    targets = [tokenizer(str(t)) for t in df['summary']]\n",
        "\n",
        "    print(\"Creating vocabulary...\")\n",
        "    all_tokens = [tok for sublist in sources+targets for tok in sublist]\n",
        "    counter = Counter(all_tokens)\n",
        "    vocab = ['xxbos', 'xxeos', 'xxpad', 'xxunk'] + [tok for tok, cnt in counter.most_common(30000)]\n",
        "    word2idx = {word: i for i, word in enumerate(vocab)}\n",
        "    idx2word = {i: w for w, i in word2idx.items()}\n",
        "    pad_idx = word2idx['xxpad']\n",
        "    unk_idx = word2idx['xxunk']\n",
        "\n",
        "    # Save vocab\n",
        "    with open(\"thai_vocab.pkl\", \"wb\") as f:\n",
        "        pickle.dump(word2idx, f)\n",
        "\n",
        "    def to_idx(seq): return [word2idx.get(w, unk_idx) for w in seq]\n",
        "    def pad_or_trim(t, length, pad_idx):\n",
        "        if len(t) < length:\n",
        "            return F.pad(t, (0, length - len(t)), value=pad_idx)\n",
        "        else:\n",
        "            return t[:length]\n",
        "\n",
        "    src_nums = [pad_or_trim(torch.tensor(to_idx(seq)), seq_len, pad_idx) for seq in sources]\n",
        "    tgt_nums = [pad_or_trim(torch.tensor(to_idx(seq)), seq_len, pad_idx) for seq in targets]\n",
        "\n",
        "    padded_src = torch.stack(src_nums)\n",
        "    padded_tgt = torch.stack(tgt_nums)\n",
        "\n",
        "    # Manual split to avoid 0 validation set\n",
        "    split_idx = int(0.8 * len(padded_src))\n",
        "    train_ds = TensorDataset(padded_src[:split_idx], padded_tgt[:split_idx])\n",
        "    valid_ds = TensorDataset(padded_src[split_idx:], padded_tgt[split_idx:])\n",
        "\n",
        "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
        "    valid_dl = DataLoader(valid_ds, batch_size=bs)\n",
        "\n",
        "    dls = DataLoaders(train_dl, valid_dl)\n",
        "\n",
        "    print(\"\\n DataLoaders created!\")\n",
        "    print(f\"- Vocab size: {len(vocab)}\")\n",
        "    print(f\"- Input shape: {padded_src[0].shape}\")\n",
        "    return dls, vocab, pad_idx\n",
        "\n",
        "dls, vocab, pad_idx = prepare_dataloaders(df)\n",
        "\n",
        "\n",
        "### 4. MODEL DEFINITION (LSTM Encoder-Decoder) #############################\n",
        "class Thai2FitSummarizer(nn.Module):\n",
        "    def __init__(self, vocab_size, pad_idx, emb_sz=300, hidden_sz=512, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, emb_sz, padding_idx=pad_idx)\n",
        "        self.encoder = nn.LSTM(emb_sz, hidden_sz, num_layers, batch_first=True)\n",
        "        self.decoder = nn.LSTM(emb_sz, hidden_sz, num_layers, batch_first=True)\n",
        "        self.out = nn.Linear(hidden_sz, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        embedded_src = self.embedding(src)\n",
        "        _, (hidden, cell) = self.encoder(embedded_src)\n",
        "\n",
        "        embedded_tgt = self.embedding(tgt)\n",
        "        output, _ = self.decoder(embedded_tgt, (hidden, cell))\n",
        "\n",
        "        logits = self.out(output)\n",
        "        return logits\n",
        "\n",
        "# Instantiate model\n",
        "model = Thai2FitSummarizer(vocab_size=len(vocab), pad_idx=pad_idx)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
        "opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "### 5. TRAINING LOOP #########################################################\n",
        "def train(model, dls, loss_fn, opt, epochs=5, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
        "    model.to(device)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for xb, yb in dls.train:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            opt.zero_grad()\n",
        "            out = model(xb, yb[:, :-1])\n",
        "            loss = loss_fn(out.reshape(-1, out.shape[-1]), yb[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        val_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in dls.valid:\n",
        "                xb, yb = xb.to(device), yb.to(device)\n",
        "                out = model(xb, yb[:, :-1])\n",
        "                loss = loss_fn(out.reshape(-1, out.shape[-1]), yb[:, 1:].reshape(-1))\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs} — Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "# Train the model\n",
        "train(model, dls, loss_fn, opt, epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_Dfr-qPGz1s",
        "outputId": "5d2e9dd4-7f05-4a25-ef5a-cae1cef292a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Informal marker 'นะ' detected in summaries\n",
            "Loaded 50 clean pairs\n",
            "Tokenizing texts...\n",
            "Creating vocabulary...\n",
            "\n",
            " DataLoaders created!\n",
            "- Vocab size: 2067\n",
            "- Input shape: torch.Size([64])\n",
            "Epoch 1/10 — Train Loss: 71.4488, Val Loss: 19.0999\n",
            "Epoch 2/10 — Train Loss: 60.7967, Val Loss: 19.2435\n",
            "Epoch 3/10 — Train Loss: 56.9685, Val Loss: 18.9287\n",
            "Epoch 4/10 — Train Loss: 53.4582, Val Loss: 18.1939\n",
            "Epoch 5/10 — Train Loss: 49.8175, Val Loss: 17.7863\n",
            "Epoch 6/10 — Train Loss: 45.9788, Val Loss: 17.0412\n",
            "Epoch 7/10 — Train Loss: 41.6886, Val Loss: 16.8431\n",
            "Epoch 8/10 — Train Loss: 37.5046, Val Loss: 16.5128\n",
            "Epoch 9/10 — Train Loss: 33.2811, Val Loss: 16.2750\n",
            "Epoch 10/10 — Train Loss: 29.1387, Val Loss: 16.0510\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def idx_to_text(idx_list, idx2word):\n",
        "    return ' '.join([idx2word.get(idx, 'xxunk') for idx in idx_list if idx != pad_idx and idx2word.get(idx) not in ['xxbos', 'xxpad']])\n",
        "\n",
        "def generate_summary(input_text, model, tokenizer, word2idx, idx2word, max_len=64, device='cpu'):\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    # Prepare input sequence\n",
        "    tokens = tokenizer(input_text)\n",
        "    input_idxs = [word2idx.get(tok, word2idx['xxunk']) for tok in tokens]\n",
        "    input_tensor = torch.tensor(input_idxs).unsqueeze(0).to(device)\n",
        "\n",
        "    # Start with BOS token\n",
        "    generated = [word2idx['xxbos']]\n",
        "\n",
        "    for _ in range(max_len):\n",
        "        tgt_tensor = torch.tensor(generated).unsqueeze(0).to(device)  # (1, current_len)\n",
        "        with torch.no_grad():\n",
        "            output = model(input_tensor, tgt_tensor)  # call forward(src, tgt)\n",
        "            next_token = output[0, -1].argmax().item()\n",
        "            generated.append(next_token)\n",
        "\n",
        "        if next_token == word2idx['xxeos']:\n",
        "            break\n",
        "\n",
        "    return idx_to_text(generated[1:], idx2word)  # skip BOS\n",
        "\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "def evaluate_model(df, model, tokenizer, word2idx, idx2word):\n",
        "    scores = []\n",
        "    for i, row in df.iterrows():\n",
        "        input_text = row['text']\n",
        "        true_summary = row['summary']\n",
        "        pred_summary = generate_summary(input_text, model, tokenizer, word2idx, idx2word)\n",
        "        rouge = scorer.score(true_summary, pred_summary)\n",
        "        scores.append(rouge)\n",
        "\n",
        "    # Average scores\n",
        "    avg_rouge1 = np.mean([s['rouge1'].fmeasure for s in scores])\n",
        "    avg_rougeL = np.mean([s['rougeL'].fmeasure for s in scores])\n",
        "    print(f\"\\n✅ ROUGE-1 F1: {avg_rouge1:.4f}, ROUGE-L F1: {avg_rougeL:.4f}\")"
      ],
      "metadata": {
        "id": "gKlqaKDMLl-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle  # Import the pickle module\n",
        "# Load vocab again if needed\n",
        "with open(\"thai_vocab.pkl\", \"rb\") as f:\n",
        "    word2idx = pickle.load(f)\n",
        "idx2word = {i: w for w, i in word2idx.items()}\n",
        "\n",
        "# Run a test inference\n",
        "sample_input = \" ด้วย ศูนย์ศึกษายุทธศาสตร์ สถาบันวิชาการป้องกันประเทศ กําหนดจัดการเสวนา แลกเปลี่ยนเรียนรู้ เรื่อง นักยุทธศาสตร์กับการพัฒนาประเทศ ในวันอังคารที่ 9 เมษายน พ.ศ. 2567 เวลา 0830-1030 ณ อาคารศูนย์นวัตกรรมการศึกษาทางทหาร สถาบันวิชาการป้องกันประเทศ รายละเอียด ตามสิ่งที่ส่งมาด้วย ในการนี้ ศูนย์ศึกษายุทธศาสตร์ฯ จึงขอเรียนเชิญผู้แทนหน่วย จํานวน 2 นาย เข้าร่วม การเสวนาแลกเปลี่ยนเรียนรู้ฯ ตามวัน และเวลาดังกล่าว ทั้งนี้ เพื่อให้การเตรียมการต้อนรับเป็นไปด้วย ความเรียบร้อย ขอความกรุณาส่งแบบตอบรับเข้าร่วมการเสวนาฯ 2 และ ขอขอบคุณมา ณ โอกาสนี้ \"\n",
        "print(\"\\n📌 Input:\", sample_input)\n",
        "print(\"📄 Summary:\", generate_summary(sample_input, model, tokenizer, word2idx, idx2word))\n",
        "\n",
        "#Evaluate full dataset\n",
        "evaluate_model(df, model, tokenizer, word2idx, idx2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HKmXa81WzUB",
        "outputId": "bd21df6e-9c49-48b9-e52a-7124613d0a87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "📌 Input:  ด้วย ศูนย์ศึกษายุทธศาสตร์ สถาบันวิชาการป้องกันประเทศ กําหนดจัดการเสวนา แลกเปลี่ยนเรียนรู้ เรื่อง นักยุทธศาสตร์กับการพัฒนาประเทศ ในวันอังคารที่ 9 เมษายน พ.ศ. 2567 เวลา 0830-1030 ณ อาคารศูนย์นวัตกรรมการศึกษาทางทหาร สถาบันวิชาการป้องกันประเทศ รายละเอียด ตามสิ่งที่ส่งมาด้วย ในการนี้ ศูนย์ศึกษายุทธศาสตร์ฯ จึงขอเรียนเชิญผู้แทนหน่วย จํานวน 2 นาย เข้าร่วม การเสวนาแลกเปลี่ยนเรียนรู้ฯ ตามวัน และเวลาดังกล่าว ทั้งนี้ เพื่อให้การเตรียมการต้อนรับเป็นไปด้วย ความเรียบร้อย ขอความกรุณาส่งแบบตอบรับเข้าร่วมการเสวนาฯ 2 และ ขอขอบคุณมา ณ โอกาสนี้ \n",
            "📄 Summary: สศท . ส ปท. ขอ เชิญ ผู้แทน หน่วย ที่ สามารถ ให้ และ เข้าร่วม ได้ ประชุม ประชุม เร่งรัด กา รด ํา เนิน งาน และ เนิน งาน งาน ตาม นโยบาย ผบ. ท สส. ประ จํา ปี 2567 ใน วัน พฤหัสบดี ที่ พ.ค. 67 เวลา 0900 ณ ห้องประชุม บก . ทท . โดย มี ผบ. ท สส. เป็น ประธาน xxeos\n",
            "\n",
            "✅ ROUGE-1 F1: 0.2400, ROUGE-L F1: 0.2400\n"
          ]
        }
      ]
    }
  ]
}